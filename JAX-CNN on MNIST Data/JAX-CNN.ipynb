{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxMUmlEENLXc"
   },
   "source": [
    "Let's first get the imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0yDWKUrCBiX"
   },
   "outputs": [],
   "source": [
    "import array\n",
    "import gzip\n",
    "import itertools\n",
    "import numpy\n",
    "import numpy.random as npr\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "from os import path\n",
    "import urllib.request\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax.api import jit, grad\n",
    "from jax.config import config\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIp--T57NGrU"
   },
   "source": [
    "The following cell contains boilerplate code to download and load MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du24u5vtDEIn"
   },
   "outputs": [],
   "source": [
    "_DATA = \"/tmp/\"\n",
    "\n",
    "def _download(url, filename):\n",
    "  \"\"\"Download a url to a file in the JAX data temp directory.\"\"\"\n",
    "  if not path.exists(_DATA):\n",
    "    os.makedirs(_DATA)\n",
    "  out_file = path.join(_DATA, filename)\n",
    "  if not path.isfile(out_file):\n",
    "    urllib.request.urlretrieve(url, out_file)\n",
    "    print(\"downloaded {} to {}\".format(url, _DATA))\n",
    "\n",
    "\n",
    "def _partial_flatten(x):\n",
    "  \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n",
    "  return numpy.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "\n",
    "def _one_hot(x, k, dtype=numpy.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return numpy.array(x[:, None] == numpy.arange(k), dtype)\n",
    "\n",
    "\n",
    "def mnist_raw():\n",
    "  \"\"\"Download and parse the raw MNIST dataset.\"\"\"\n",
    "  # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
    "  base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "\n",
    "  def parse_labels(filename):\n",
    "    with gzip.open(filename, \"rb\") as fh:\n",
    "      _ = struct.unpack(\">II\", fh.read(8))\n",
    "      return numpy.array(array.array(\"B\", fh.read()), dtype=numpy.uint8)\n",
    "\n",
    "  def parse_images(filename):\n",
    "    with gzip.open(filename, \"rb\") as fh:\n",
    "      _, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
    "      return numpy.array(array.array(\"B\", fh.read()),\n",
    "                      dtype=numpy.uint8).reshape(num_data, rows, cols)\n",
    "\n",
    "  for filename in [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
    "                   \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]:\n",
    "    _download(base_url + filename, filename)\n",
    "\n",
    "  train_images = parse_images(path.join(_DATA, \"train-images-idx3-ubyte.gz\"))\n",
    "  train_labels = parse_labels(path.join(_DATA, \"train-labels-idx1-ubyte.gz\"))\n",
    "  test_images = parse_images(path.join(_DATA, \"t10k-images-idx3-ubyte.gz\"))\n",
    "  test_labels = parse_labels(path.join(_DATA, \"t10k-labels-idx1-ubyte.gz\"))\n",
    "\n",
    "  return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "def mnist(create_outliers=False):\n",
    "  \"\"\"Download, parse and process MNIST data to unit scale and one-hot labels.\"\"\"\n",
    "  train_images, train_labels, test_images, test_labels = mnist_raw()\n",
    "\n",
    "  train_images = _partial_flatten(train_images) / numpy.float32(255.)\n",
    "  test_images = _partial_flatten(test_images) / numpy.float32(255.)\n",
    "  train_labels = _one_hot(train_labels, 10)\n",
    "  test_labels = _one_hot(test_labels, 10)\n",
    "\n",
    "  if create_outliers:\n",
    "    mum_outliers = 30000\n",
    "    perm = numpy.random.RandomState(0).permutation(mum_outliers)\n",
    "    train_images[:mum_outliers] = train_images[:mum_outliers][perm]\n",
    "\n",
    "  return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "def shape_as_image(images, labels, dummy_dim=False):\n",
    "  target_shape = (-1, 1, 28, 28, 1) if dummy_dim else (-1, 28, 28, 1)\n",
    "  return np.reshape(images, target_shape), labels\n",
    "\n",
    "train_images, train_labels, test_images, test_labels = mnist(create_outliers=False)\n",
    "num_train = train_images.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdTKppnqIHeV"
   },
   "source": [
    "# **Problem 1**\n",
    "\n",
    "This function computes the output of a fully-connected neural network (i.e., multilayer perceptron) by iterating over all of its layers and:\n",
    "\n",
    "1. taking the `activations` of the previous layer (or the input itself for the first hidden layer) to compute the `outputs` of a linear classifier. Recall the lectures: `outputs` is what we wrote $z=w\\cdot x + b$ where $x$ is the input to the linear classifier. \n",
    "2. applying a non-linear activation. Here we will use $tanh$.\n",
    "\n",
    "Complete the following cell to compute `outputs` and `activations`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGU4xK-iHjhx"
   },
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "  activations = inputs\n",
    "  for w, b in params[:-1]: #iterate through each layer\n",
    "    outputs = np.dot(activations, w) + b\n",
    "    activations = np.tanh(outputs)\n",
    "\n",
    "  final_w, final_b = params[-1]\n",
    "  logits = np.dot(activations, final_w) + final_b #Return a matrix, each vector is one sample\n",
    "  return logits - logsumexp(logits, axis=1, keepdims=True) #Return a matrix, contains different vectors, each vector has element with log-logsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgE_NBb9JNDI"
   },
   "source": [
    "The following cell computes the loss of our model. Here we are using cross-entropy combined with a softmax but the implementation uses the `LogSumExp` trick for numerical stability. This is why our previous function `predict` returns the logits to which we substract the `logsumexp` of logits. We discussed this in class but you can read more about it [here](https://blog.feedly.com/tricks-of-the-trade-logsumexp/).\n",
    "\n",
    "Complete the return line. Recall that the loss is defined as :\n",
    "$$ l(X, Y) = -\\frac{1}{n} \\sum_{i\\in 1..n}  \\sum_{j\\in 1.. K}y_j^{(i)} \\log(f_j(x^{(i)})) = -\\frac{1}{n} \\sum_{i\\in 1..n}  \\sum_{j\\in 1.. K}y_j^{(i)} \\log\\left(\\frac{z_j^{(i)}}{\\sum_{k\\in 1..K}z_k^{(i)}}\\right) $$\n",
    "where $X$ is a matrix containing a batch of $n$ training inputs, and $Y$ a matrix containing a batch of one-hot encoded labels defined over $K$ labels. Here $z_j^{(i)}$ is the logits (i.e., input to the softmax) of the model on the example $i$ of our batch of training examples $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlgdP72hH9ly"
   },
   "outputs": [],
   "source": [
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  preds = predict(params, inputs)\n",
    "  return -np.mean(np.sum(preds * targets, axis=1)) #-np.mean(np.sum(preds * targets, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILZ-q5PTMohU"
   },
   "source": [
    "The following cell defines the accuracy of our model and how to initialize its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDjCuIGzIAjf"
   },
   "outputs": [],
   "source": [
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == target_class) #Return the % of same classes, array_1([0, 1, 2, 3]) , array_2([0, 1, 2, 0]), np.mean(array_1 == array_2) = 0.75\n",
    "\n",
    "def init_random_params(layer_sizes, rng=npr.RandomState(0)): #Layer sizes contain number of neurons in each layer\n",
    "  scale = 0.1\n",
    "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "as46_qd5NWMA"
   },
   "source": [
    "The following line defines our architecture with the number of neurons contained in each fully-connected layer (the first layer has 784 neurons because MNIST images are 28*28=784 pixels and the last layer has 10 neurons because MNIST has 10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZzv-4dHNV4D"
   },
   "outputs": [],
   "source": [
    "layer_sizes = [784, 1024, 128, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKroKJ6TOETY"
   },
   "source": [
    "The following cell creates a Python generator for our dataset. It outputs one batch of $n$ training examples at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6lLT1klOMIn"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size) #num_train = 60000, batch_size = 128, returns quotient and remainder. 468, 96\n",
    "num_batches = num_complete_batches + bool(leftover) #num_batches = 469\n",
    "\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train) #1-60000 randomly permutate\n",
    "    for i in range(num_batches): #in range(469)\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size] #take 128 samples\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lm-UbcYZOOci"
   },
   "source": [
    "We are now ready to define our optimizer. Here we use mini-batch stochastic gradient descent. Complete `<w UPDATE RULE>` and `<b UPDATE RULE>` using the update rule we saw in class. Recall that `dw` is the partial derivative of the `loss` with respect to `w` and `learning_rate` is the learning rate of gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8EktCm-OvFh"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "@jit\n",
    "def update(params, batch):\n",
    "  grads = grad(loss)(params, batch)\n",
    "  return [(w - learning_rate * dw, b - learning_rate * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tqo4M7uNOvzo"
   },
   "source": [
    "This is now the proper training loop for our fully-connected neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "fgrHTrafDHAE",
    "outputId": "578aa412-3aa8-44e3-f03e-0e41e87fb3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 2.73 sec\n",
      "Training set accuracy 0.9401500225067139\n",
      "Test set accuracy 0.9377000331878662\n",
      "Epoch 1 in 0.49 sec\n",
      "Training set accuracy 0.9592833518981934\n",
      "Test set accuracy 0.95250004529953\n",
      "Epoch 2 in 0.50 sec\n",
      "Training set accuracy 0.9681666493415833\n",
      "Test set accuracy 0.9607000350952148\n",
      "Epoch 3 in 0.48 sec\n",
      "Training set accuracy 0.9759166836738586\n",
      "Test set accuracy 0.9663000702857971\n",
      "Epoch 4 in 0.49 sec\n",
      "Training set accuracy 0.9795500040054321\n",
      "Test set accuracy 0.9676000475883484\n",
      "Epoch 5 in 0.49 sec\n",
      "Training set accuracy 0.982616662979126\n",
      "Test set accuracy 0.970300018787384\n",
      "Epoch 6 in 0.50 sec\n",
      "Training set accuracy 0.9865833520889282\n",
      "Test set accuracy 0.9716000556945801\n",
      "Epoch 7 in 0.50 sec\n",
      "Training set accuracy 0.9892333149909973\n",
      "Test set accuracy 0.9736000299453735\n",
      "Epoch 8 in 0.50 sec\n",
      "Training set accuracy 0.9911666512489319\n",
      "Test set accuracy 0.9741000533103943\n",
      "Epoch 9 in 0.51 sec\n",
      "Training set accuracy 0.992983341217041\n",
      "Test set accuracy 0.9746000170707703\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "params = init_random_params(layer_sizes)\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for _ in range(num_batches):\n",
    "    params = update(params, next(batches))  #next() returns the next iterm from iteration\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, (train_images, train_labels))\n",
    "  test_acc = accuracy(params, (test_images, test_labels))\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geEOW6BLcdGv"
   },
   "source": [
    "### **Slow Convergence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "y6BGtKRTa8en",
    "outputId": "0767332f-d99a-4a09-bc16-658cd01c3664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 1.26 sec\n",
      "Training set accuracy 0.5794000029563904\n",
      "Test set accuracy 0.5819000005722046\n",
      "Epoch 1 in 0.47 sec\n",
      "Training set accuracy 0.7056000232696533\n",
      "Test set accuracy 0.7110000252723694\n",
      "Epoch 2 in 0.50 sec\n",
      "Training set accuracy 0.760366678237915\n",
      "Test set accuracy 0.7656000256538391\n",
      "Epoch 3 in 0.49 sec\n",
      "Training set accuracy 0.7911666631698608\n",
      "Test set accuracy 0.7988000512123108\n",
      "Epoch 4 in 0.52 sec\n",
      "Training set accuracy 0.8119333386421204\n",
      "Test set accuracy 0.8196000456809998\n",
      "Epoch 5 in 0.49 sec\n",
      "Training set accuracy 0.8274999856948853\n",
      "Test set accuracy 0.8343000411987305\n",
      "Epoch 6 in 0.50 sec\n",
      "Training set accuracy 0.8385833501815796\n",
      "Test set accuracy 0.8464000225067139\n",
      "Epoch 7 in 0.50 sec\n",
      "Training set accuracy 0.8474000096321106\n",
      "Test set accuracy 0.8584000468254089\n",
      "Epoch 8 in 0.50 sec\n",
      "Training set accuracy 0.8540499806404114\n",
      "Test set accuracy 0.8649000525474548\n",
      "Epoch 9 in 0.52 sec\n",
      "Training set accuracy 0.8603500127792358\n",
      "Test set accuracy 0.8695000410079956\n"
     ]
    }
   ],
   "source": [
    "def predict(params, inputs):\n",
    "  activations = inputs\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = np.dot(activations, w) + b\n",
    "    activations = np.tanh(outputs)\n",
    "\n",
    "  final_w, final_b = params[-1]\n",
    "  logits = np.dot(activations, final_w) + final_b\n",
    "  return logits - logsumexp(logits, axis=1, keepdims=True)\n",
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  preds = predict(params, inputs)\n",
    "  return -np.mean(np.sum(preds * targets, axis=1))\n",
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == target_class)\n",
    "def init_random_params(layer_sizes, rng=npr.RandomState(0)):\n",
    "  scale = 0.1\n",
    "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "layer_sizes = [784, 1024, 128, 10]\n",
    "batch_size = 128\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size) #num_train = 60000, batch_size = 128, returns quotient and remainder. 468, 96\n",
    "num_batches = num_complete_batches + bool(leftover) #num_batches = 469\n",
    "\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train) #1-60000 randomly permutate\n",
    "    for i in range(num_batches): #in range(469)\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size] #take 128 samples\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()\n",
    "learning_rate = 0.001\n",
    "\n",
    "@jit\n",
    "def update(params, batch):\n",
    "  grads = grad(loss)(params, batch)\n",
    "  return [(w - learning_rate * dw, b - learning_rate * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]\n",
    "num_epochs = 10\n",
    "params = init_random_params(layer_sizes)\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for _ in range(num_batches):\n",
    "    params = update(params, next(batches))  #next() returns the next iterm from iteration\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, (train_images, train_labels))\n",
    "  test_acc = accuracy(params, (test_images, test_labels))\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KexJZG2csN-"
   },
   "source": [
    "### **Oscillations but still converges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "dEQcvyTkc8Yx",
    "outputId": "a4b32bcb-1a4b-4c40-8f75-8c472d8fce9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 2.73 sec\n",
      "Training set accuracy 0.8037166595458984\n",
      "Test set accuracy 0.8108000159263611\n",
      "Epoch 1 in 0.51 sec\n",
      "Training set accuracy 0.8496833443641663\n",
      "Test set accuracy 0.8445000648498535\n",
      "Epoch 2 in 0.51 sec\n",
      "Training set accuracy 0.8338333368301392\n",
      "Test set accuracy 0.8315000534057617\n",
      "Epoch 3 in 0.52 sec\n",
      "Training set accuracy 0.9210333228111267\n",
      "Test set accuracy 0.9182000160217285\n",
      "Epoch 4 in 0.50 sec\n",
      "Training set accuracy 0.9023500084877014\n",
      "Test set accuracy 0.8945000171661377\n",
      "Epoch 5 in 0.51 sec\n",
      "Training set accuracy 0.8478666543960571\n",
      "Test set accuracy 0.844700038433075\n",
      "Epoch 6 in 0.47 sec\n",
      "Training set accuracy 0.9323333501815796\n",
      "Test set accuracy 0.9280000329017639\n",
      "Epoch 7 in 0.52 sec\n",
      "Training set accuracy 0.9317333698272705\n",
      "Test set accuracy 0.9284000396728516\n",
      "Epoch 8 in 0.52 sec\n",
      "Training set accuracy 0.935533344745636\n",
      "Test set accuracy 0.9326000213623047\n",
      "Epoch 9 in 0.48 sec\n",
      "Training set accuracy 0.956083357334137\n",
      "Test set accuracy 0.9481000304222107\n"
     ]
    }
   ],
   "source": [
    "def predict(params, inputs):\n",
    "  activations = inputs\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = np.dot(activations, w) + b\n",
    "    activations = np.tanh(outputs)\n",
    "\n",
    "  final_w, final_b = params[-1]\n",
    "  logits = np.dot(activations, final_w) + final_b\n",
    "  return logits - logsumexp(logits, axis=1, keepdims=True)\n",
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  preds = predict(params, inputs)\n",
    "  return -np.mean(np.sum(preds * targets, axis=1))\n",
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == target_class)\n",
    "def init_random_params(layer_sizes, rng=npr.RandomState(0)):\n",
    "  scale = 0.1\n",
    "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "layer_sizes = [784, 1024, 128, 10]\n",
    "batch_size = 128\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size) #num_train = 60000, batch_size = 128, returns quotient and remainder. 468, 96\n",
    "num_batches = num_complete_batches + bool(leftover) #num_batches = 469\n",
    "\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train) #1-60000 randomly permutate\n",
    "    for i in range(num_batches): #in range(469)\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size] #take 128 samples\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()\n",
    "learning_rate = 1.6\n",
    "\n",
    "@jit\n",
    "def update(params, batch):\n",
    "  grads = grad(loss)(params, batch)\n",
    "  return [(w - learning_rate * dw, b - learning_rate * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]\n",
    "num_epochs = 10\n",
    "params = init_random_params(layer_sizes)\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for _ in range(num_batches):\n",
    "    params = update(params, next(batches))  #next() returns the next iterm from iteration\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, (train_images, train_labels))\n",
    "  test_acc = accuracy(params, (test_images, test_labels))\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4h3XXhmYdQkF"
   },
   "source": [
    "### **Instability and diverges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "woSogcwadWv9",
    "outputId": "bed7834a-f150-46ac-ad5a-17dcce0a3cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 2.68 sec\n",
      "Training set accuracy 0.09035000205039978\n",
      "Test set accuracy 0.08920000493526459\n",
      "Epoch 1 in 0.54 sec\n",
      "Training set accuracy 0.10441666841506958\n",
      "Test set accuracy 0.10280000418424606\n",
      "Epoch 2 in 0.52 sec\n",
      "Training set accuracy 0.09863333404064178\n",
      "Test set accuracy 0.0958000048995018\n",
      "Epoch 3 in 0.54 sec\n",
      "Training set accuracy 0.09930000454187393\n",
      "Test set accuracy 0.10320000350475311\n",
      "Epoch 4 in 0.56 sec\n",
      "Training set accuracy 0.10441666841506958\n",
      "Test set accuracy 0.10280000418424606\n",
      "Epoch 5 in 0.57 sec\n",
      "Training set accuracy 0.09930000454187393\n",
      "Test set accuracy 0.10320000350475311\n",
      "Epoch 6 in 0.55 sec\n",
      "Training set accuracy 0.09871666878461838\n",
      "Test set accuracy 0.09800000488758087\n",
      "Epoch 7 in 0.56 sec\n",
      "Training set accuracy 0.09863333404064178\n",
      "Test set accuracy 0.0958000048995018\n",
      "Epoch 8 in 0.58 sec\n",
      "Training set accuracy 0.09736666828393936\n",
      "Test set accuracy 0.0982000082731247\n",
      "Epoch 9 in 0.55 sec\n",
      "Training set accuracy 0.10218333452939987\n",
      "Test set accuracy 0.10100000351667404\n"
     ]
    }
   ],
   "source": [
    "def predict(params, inputs):\n",
    "  activations = inputs\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = np.dot(activations, w) + b\n",
    "    activations = np.tanh(outputs)\n",
    "\n",
    "  final_w, final_b = params[-1]\n",
    "  logits = np.dot(activations, final_w) + final_b\n",
    "  return logits - logsumexp(logits, axis=1, keepdims=True)\n",
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  preds = predict(params, inputs)\n",
    "  return -np.mean(np.sum(preds * targets, axis=1))\n",
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == target_class)\n",
    "def init_random_params(layer_sizes, rng=npr.RandomState(0)):\n",
    "  scale = 0.1\n",
    "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "layer_sizes = [784, 1024, 128, 10]\n",
    "batch_size = 128\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size) #num_train = 60000, batch_size = 128, returns quotient and remainder. 468, 96\n",
    "num_batches = num_complete_batches + bool(leftover) #num_batches = 469\n",
    "\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train) #1-60000 randomly permutate\n",
    "    for i in range(num_batches): #in range(469)\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size] #take 128 samples\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()\n",
    "learning_rate = 2\n",
    "\n",
    "@jit\n",
    "def update(params, batch):\n",
    "  grads = grad(loss)(params, batch)\n",
    "  return [(w - learning_rate * dw, b - learning_rate * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]\n",
    "num_epochs = 10\n",
    "params = init_random_params(layer_sizes)\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for _ in range(num_batches):\n",
    "    params = update(params, next(batches))  #next() returns the next iterm from iteration\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, (train_images, train_labels))\n",
    "  test_acc = accuracy(params, (test_images, test_labels))\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghe4DGWaR46y"
   },
   "source": [
    "### **Underfit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "oVU7VwkaNdBK",
    "outputId": "16246648-351f-43ae-f2c4-0c727493e8aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 2.47 sec\n",
      "Training set accuracy 0.6809833645820618\n",
      "Test set accuracy 0.6829000115394592\n",
      "Epoch 1 in 0.48 sec\n",
      "Training set accuracy 0.7415333390235901\n",
      "Test set accuracy 0.7386000156402588\n",
      "Epoch 2 in 0.48 sec\n",
      "Training set accuracy 0.7940000295639038\n",
      "Test set accuracy 0.7928000092506409\n",
      "Epoch 3 in 0.45 sec\n",
      "Training set accuracy 0.8138499855995178\n",
      "Test set accuracy 0.8131000399589539\n",
      "Epoch 4 in 0.47 sec\n",
      "Training set accuracy 0.8187167048454285\n",
      "Test set accuracy 0.8192000389099121\n",
      "Epoch 5 in 0.47 sec\n",
      "Training set accuracy 0.8266833424568176\n",
      "Test set accuracy 0.8264000415802002\n",
      "Epoch 6 in 0.48 sec\n",
      "Training set accuracy 0.8317500352859497\n",
      "Test set accuracy 0.8302000164985657\n",
      "Epoch 7 in 0.46 sec\n",
      "Training set accuracy 0.8362333178520203\n",
      "Test set accuracy 0.8311000466346741\n",
      "Epoch 8 in 0.49 sec\n",
      "Training set accuracy 0.840499997138977\n",
      "Test set accuracy 0.8371000289916992\n",
      "Epoch 9 in 0.50 sec\n",
      "Training set accuracy 0.8401833176612854\n",
      "Test set accuracy 0.834100067615509\n"
     ]
    }
   ],
   "source": [
    "def predict(params, inputs):\n",
    "  activations = inputs\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = np.dot(activations, w) + b\n",
    "    activations = np.tanh(outputs)\n",
    "\n",
    "  final_w, final_b = params[-1]\n",
    "  logits = np.dot(activations, final_w) + final_b\n",
    "  return logits - logsumexp(logits, axis=1, keepdims=True)\n",
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  preds = predict(params, inputs)\n",
    "  return -np.mean(np.sum(preds * targets, axis=1))\n",
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == target_class)\n",
    "def init_random_params(layer_sizes, rng=npr.RandomState(0)):\n",
    "  scale = 0.1\n",
    "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "layer_sizes = [784, 4, 10]\n",
    "batch_size = 128\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train)\n",
    "    for i in range(num_batches):\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()\n",
    "learning_rate = 0.1\n",
    "\n",
    "@jit\n",
    "def update(params, batch):\n",
    "  grads = grad(loss)(params, batch)\n",
    "  return [(w - learning_rate * dw, b - learning_rate * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "num_epochs = 10\n",
    "params = init_random_params(layer_sizes)\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for _ in range(num_batches):\n",
    "    params = update(params, next(batches))\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, (train_images, train_labels))\n",
    "  test_acc = accuracy(params, (test_images, test_labels))\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jYyuqCOR8Vr"
   },
   "source": [
    "### **Overfit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XwFfq7lLR-Rv",
    "outputId": "220c8a9b-aa81-449c-b4fb-d1cff99bc091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 3.86 sec\n",
      "Training set accuracy 0.45848333835601807\n",
      "Test set accuracy 0.7910000085830688\n",
      "Epoch 1 in 0.81 sec\n",
      "Training set accuracy 0.49258333444595337\n",
      "Test set accuracy 0.8338000178337097\n",
      "Epoch 2 in 0.82 sec\n",
      "Training set accuracy 0.5123833417892456\n",
      "Test set accuracy 0.8500000238418579\n",
      "Epoch 3 in 0.82 sec\n",
      "Training set accuracy 0.5232999920845032\n",
      "Test set accuracy 0.8551000356674194\n",
      "Epoch 4 in 0.82 sec\n",
      "Training set accuracy 0.5355499982833862\n",
      "Test set accuracy 0.8591000437736511\n",
      "Epoch 5 in 0.83 sec\n",
      "Training set accuracy 0.54708331823349\n",
      "Test set accuracy 0.8568000197410583\n",
      "Epoch 6 in 0.82 sec\n",
      "Training set accuracy 0.5604333281517029\n",
      "Test set accuracy 0.851900041103363\n",
      "Epoch 7 in 0.83 sec\n",
      "Training set accuracy 0.5738000273704529\n",
      "Test set accuracy 0.8452000617980957\n",
      "Epoch 8 in 0.83 sec\n",
      "Training set accuracy 0.5889166593551636\n",
      "Test set accuracy 0.8370000123977661\n",
      "Epoch 9 in 0.83 sec\n",
      "Training set accuracy 0.6087499856948853\n",
      "Test set accuracy 0.8294000625610352\n",
      "Epoch 10 in 0.83 sec\n",
      "Training set accuracy 0.6301666498184204\n",
      "Test set accuracy 0.8176000118255615\n",
      "Epoch 11 in 0.83 sec\n",
      "Training set accuracy 0.6569666862487793\n",
      "Test set accuracy 0.7988000512123108\n",
      "Epoch 12 in 0.83 sec\n",
      "Training set accuracy 0.6810666918754578\n",
      "Test set accuracy 0.791100025177002\n",
      "Epoch 13 in 0.83 sec\n",
      "Training set accuracy 0.7076666951179504\n",
      "Test set accuracy 0.7793000340461731\n",
      "Epoch 14 in 0.83 sec\n",
      "Training set accuracy 0.7372333407402039\n",
      "Test set accuracy 0.7653000354766846\n",
      "Epoch 15 in 0.83 sec\n",
      "Training set accuracy 0.7625166773796082\n",
      "Test set accuracy 0.7528000473976135\n",
      "Epoch 16 in 0.83 sec\n",
      "Training set accuracy 0.7967166900634766\n",
      "Test set accuracy 0.7314000129699707\n",
      "Epoch 17 in 0.83 sec\n",
      "Training set accuracy 0.8203833699226379\n",
      "Test set accuracy 0.7214000225067139\n",
      "Epoch 18 in 0.83 sec\n",
      "Training set accuracy 0.8525333404541016\n",
      "Test set accuracy 0.6990000605583191\n",
      "Epoch 19 in 0.84 sec\n",
      "Training set accuracy 0.8761166930198669\n",
      "Test set accuracy 0.6940000057220459\n",
      "Epoch 20 in 0.83 sec\n",
      "Training set accuracy 0.8943666815757751\n",
      "Test set accuracy 0.6793000102043152\n",
      "Epoch 21 in 0.84 sec\n",
      "Training set accuracy 0.9128167033195496\n",
      "Test set accuracy 0.6815000176429749\n",
      "Epoch 22 in 0.84 sec\n",
      "Training set accuracy 0.9319666624069214\n",
      "Test set accuracy 0.6716000437736511\n",
      "Epoch 23 in 0.84 sec\n",
      "Training set accuracy 0.9478333592414856\n",
      "Test set accuracy 0.6634000539779663\n",
      "Epoch 24 in 0.84 sec\n",
      "Training set accuracy 0.9574166536331177\n",
      "Test set accuracy 0.6579000353813171\n",
      "Epoch 25 in 0.84 sec\n",
      "Training set accuracy 0.9663833379745483\n",
      "Test set accuracy 0.6503000259399414\n",
      "Epoch 26 in 0.84 sec\n",
      "Training set accuracy 0.9766333699226379\n",
      "Test set accuracy 0.6508000493049622\n",
      "Epoch 27 in 0.84 sec\n",
      "Training set accuracy 0.9809166789054871\n",
      "Test set accuracy 0.6421000361442566\n",
      "Epoch 28 in 0.84 sec\n",
      "Training set accuracy 0.9860833287239075\n",
      "Test set accuracy 0.6406000256538391\n",
      "Epoch 29 in 0.84 sec\n",
      "Training set accuracy 0.9897833466529846\n",
      "Test set accuracy 0.6451000571250916\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = mnist(create_outliers=True)\n",
    "num_train = train_images.shape[0]\n",
    "\n",
    "def predict(params, inputs):\n",
    "  activations = inputs\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = np.dot(activations, w) + b\n",
    "    activations = np.tanh(outputs)\n",
    "\n",
    "  final_w, final_b = params[-1]\n",
    "  logits = np.dot(activations, final_w) + final_b\n",
    "  return logits - logsumexp(logits, axis=1, keepdims=True)\n",
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  preds = predict(params, inputs)\n",
    "  return -np.mean(np.sum(preds * targets, axis=1))\n",
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == target_class)\n",
    "def init_random_params(layer_sizes, rng=npr.RandomState(0)):\n",
    "  scale = 0.1\n",
    "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
    "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "layer_sizes = [784, 2048, 1024, 512, 1024, 512, 128, 10]\n",
    "batch_size = 128\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train)\n",
    "    for i in range(num_batches):\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()\n",
    "learning_rate = 0.01\n",
    "\n",
    "@jit\n",
    "def update(params, batch):\n",
    "  grads = grad(loss)(params, batch)\n",
    "  return [(w - learning_rate * dw, b - learning_rate * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "num_epochs = 30\n",
    "params = init_random_params(layer_sizes)\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for _ in range(num_batches):\n",
    "    params = update(params, next(batches))\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, (train_images, train_labels))\n",
    "  test_acc = accuracy(params, (test_images, test_labels))\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9yFvjncWk8G"
   },
   "source": [
    "# **Problem 2**\n",
    "\n",
    "Before we get started, we need to import two small libraries that contain boilerplate code for common neural network layer types and for optimizers like mini-batch SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvLxNfXtXCRb"
   },
   "outputs": [],
   "source": [
    "from jax.experimental import optimizers\n",
    "from jax.experimental import stax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNwMlXqfXI8G"
   },
   "source": [
    "Here is a fully-connected neural network architecture, like the one of Problem 1, but this time defined with `stax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4wu1XqFds4X"
   },
   "outputs": [],
   "source": [
    "init_random_params, predict = stax.serial(\n",
    "    stax.Flatten,\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(128),\n",
    "    stax.Relu,\n",
    "    stax.Dense(10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEW_OcOCdwFX"
   },
   "source": [
    "We redefine the cross-entropy loss for this model. As done in Problem 1, complete the return line below (it's identical). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQEeOtAEdvYn"
   },
   "outputs": [],
   "source": [
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  logits = predict(params, inputs)\n",
    "  preds  = stax.logsoftmax(logits)\n",
    "  return -np.mean(np.sum(preds * targets, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZBxTvCweJbN"
   },
   "source": [
    "Next, we define the mini-batch SGD optimizer, this time with the optimizers library in JAX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peG-cAZ0eGTG"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.15\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "\n",
    "@jit\n",
    "def update(_, i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  return opt_update(i, grad(loss)(params, batch), opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVx2h8lqeoTD"
   },
   "source": [
    "The next cell contains our training loop, very similar to Problem 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "41Y6wwFzb-mk",
    "outputId": "4fccc1c2-3843-4738-8ad6-1ed9cc33f963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Test set loss, accuracy (%): (0.18, 94.58)\n",
      "Epoch 2 Test set loss, accuracy (%): (0.12, 96.46)\n",
      "Epoch 3 Test set loss, accuracy (%): (0.10, 96.95)\n",
      "Epoch 4 Test set loss, accuracy (%): (0.08, 97.57)\n",
      "Epoch 5 Test set loss, accuracy (%): (0.08, 97.64)\n",
      "Epoch 6 Test set loss, accuracy (%): (0.06, 97.84)\n",
      "Epoch 7 Test set loss, accuracy (%): (0.07, 97.88)\n",
      "Epoch 8 Test set loss, accuracy (%): (0.06, 98.02)\n",
      "Epoch 9 Test set loss, accuracy (%): (0.08, 97.68)\n",
      "Epoch 10 Test set loss, accuracy (%): (0.06, 98.09)\n",
      "Epoch 11 Test set loss, accuracy (%): (0.06, 98.07)\n",
      "Epoch 12 Test set loss, accuracy (%): (0.07, 97.93)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 12\n",
    "\n",
    "key = random.PRNGKey(123)\n",
    "_, init_params = init_random_params(key, (-1, 28, 28, 1))\n",
    "opt_state = opt_init(init_params)\n",
    "itercount = itertools.count()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  for _ in range(num_batches):\n",
    "    opt_state = update(key, next(itercount), opt_state, shape_as_image(*next(batches)))\n",
    "\n",
    "  params = get_params(opt_state)\n",
    "  test_acc = accuracy(params, shape_as_image(test_images, test_labels))\n",
    "  test_loss = loss(params, shape_as_image(test_images, test_labels))\n",
    "  print('Epoch {} Test set loss, accuracy (%): ({:.2f}, {:.2f})'.format(epoch, test_loss, 100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4f15UrxWaSZ"
   },
   "source": [
    "### **Convnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXl8A4xCiSaE"
   },
   "outputs": [],
   "source": [
    "init_random_params, predict = stax.serial(\n",
    "    stax.Conv(out_chan=12, filter_shape=(5, 5), strides=(1, 1)),\n",
    "    stax.Relu,\n",
    "    stax.MaxPool(window_shape=(2, 2)),\n",
    "    stax.Conv(out_chan=16, filter_shape=(5, 5), strides=(1, 1)),\n",
    "    stax.Relu,\n",
    "    stax.MaxPool(window_shape=(2, 2)),\n",
    "    stax.Flatten,\n",
    "    stax.Dense(120),\n",
    "    stax.Relu,\n",
    "    stax.Dense(84),\n",
    "    stax.Relu,\n",
    "    stax.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h26ZQbAHjF38"
   },
   "outputs": [],
   "source": [
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  logits = predict(params, inputs)\n",
    "  preds  = stax.logsoftmax(logits)\n",
    "  return -np.mean(np.sum(preds * targets, axis=1))\n",
    "\n",
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  target_class = np.argmax(targets, axis=1)\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbBu3M4GjH9e"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.06\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "\n",
    "@jit\n",
    "def update(_, i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  return opt_update(i, grad(loss)(params, batch), opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MTTjf4qgjKcJ",
    "outputId": "2013f86e-f33a-4cdd-a849-558422669531",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Epoch 1 Test set loss, accuracy (%): (0.07, 97.79)\n",
      "Epoch 2 Test set loss, accuracy (%): (0.04, 98.48)\n",
      "Epoch 3 Test set loss, accuracy (%): (0.04, 98.77)\n",
      "Epoch 4 Test set loss, accuracy (%): (0.03, 98.95)\n",
      "Epoch 5 Test set loss, accuracy (%): (0.03, 98.90)\n",
      "Epoch 6 Test set loss, accuracy (%): (0.03, 99.07)\n",
      "Epoch 7 Test set loss, accuracy (%): (0.03, 99.05)\n",
      "Epoch 8 Test set loss, accuracy (%): (0.04, 98.80)\n",
      "Epoch 9 Test set loss, accuracy (%): (0.05, 98.39)\n",
      "Epoch 10 Test set loss, accuracy (%): (0.03, 99.12)\n",
      "Epoch 11 Test set loss, accuracy (%): (0.04, 98.75)\n",
      "Epoch 12 Test set loss, accuracy (%): (0.04, 98.85)\n",
      "Epoch 13 Test set loss, accuracy (%): (0.03, 99.09)\n",
      "Epoch 14 Test set loss, accuracy (%): (0.03, 99.14)\n",
      "Epoch 15 Test set loss, accuracy (%): (0.03, 99.24)\n",
      "Epoch 16 Test set loss, accuracy (%): (0.03, 99.08)\n",
      "Epoch 17 Test set loss, accuracy (%): (0.03, 99.19)\n",
      "Epoch 18 Test set loss, accuracy (%): (0.03, 99.19)\n",
      "Epoch 19 Test set loss, accuracy (%): (0.03, 99.20)\n",
      "Epoch 20 Test set loss, accuracy (%): (0.03, 99.22)\n",
      "Run 2\n",
      "Epoch 1 Test set loss, accuracy (%): (0.07, 97.78)\n",
      "Epoch 2 Test set loss, accuracy (%): (0.04, 98.67)\n",
      "Epoch 3 Test set loss, accuracy (%): (0.04, 98.86)\n",
      "Epoch 4 Test set loss, accuracy (%): (0.03, 98.92)\n",
      "Epoch 5 Test set loss, accuracy (%): (0.04, 98.86)\n",
      "Epoch 6 Test set loss, accuracy (%): (0.03, 99.02)\n",
      "Epoch 7 Test set loss, accuracy (%): (0.03, 99.04)\n",
      "Epoch 8 Test set loss, accuracy (%): (0.03, 98.99)\n",
      "Epoch 9 Test set loss, accuracy (%): (0.03, 98.95)\n",
      "Epoch 10 Test set loss, accuracy (%): (0.03, 99.08)\n",
      "Epoch 11 Test set loss, accuracy (%): (0.03, 98.97)\n",
      "Epoch 12 Test set loss, accuracy (%): (0.04, 98.74)\n",
      "Epoch 13 Test set loss, accuracy (%): (0.04, 98.87)\n",
      "Epoch 14 Test set loss, accuracy (%): (0.03, 99.09)\n",
      "Epoch 15 Test set loss, accuracy (%): (0.03, 99.09)\n",
      "Epoch 16 Test set loss, accuracy (%): (0.03, 99.13)\n",
      "Epoch 17 Test set loss, accuracy (%): (0.04, 99.10)\n",
      "Epoch 18 Test set loss, accuracy (%): (0.03, 99.11)\n",
      "Epoch 19 Test set loss, accuracy (%): (0.03, 99.07)\n",
      "Epoch 20 Test set loss, accuracy (%): (0.03, 99.06)\n",
      "Run 3\n",
      "Epoch 1 Test set loss, accuracy (%): (0.07, 97.89)\n",
      "Epoch 2 Test set loss, accuracy (%): (0.04, 98.46)\n",
      "Epoch 3 Test set loss, accuracy (%): (0.04, 98.79)\n",
      "Epoch 4 Test set loss, accuracy (%): (0.03, 98.91)\n",
      "Epoch 5 Test set loss, accuracy (%): (0.03, 98.86)\n",
      "Epoch 6 Test set loss, accuracy (%): (0.03, 98.92)\n",
      "Epoch 7 Test set loss, accuracy (%): (0.03, 99.00)\n",
      "Epoch 8 Test set loss, accuracy (%): (0.04, 98.79)\n",
      "Epoch 9 Test set loss, accuracy (%): (0.03, 99.06)\n",
      "Epoch 10 Test set loss, accuracy (%): (0.03, 99.14)\n",
      "Epoch 11 Test set loss, accuracy (%): (0.04, 98.74)\n",
      "Epoch 12 Test set loss, accuracy (%): (0.05, 98.39)\n",
      "Epoch 13 Test set loss, accuracy (%): (0.04, 98.78)\n",
      "Epoch 14 Test set loss, accuracy (%): (0.04, 98.82)\n",
      "Epoch 15 Test set loss, accuracy (%): (0.03, 99.14)\n",
      "Epoch 16 Test set loss, accuracy (%): (0.03, 99.15)\n",
      "Epoch 17 Test set loss, accuracy (%): (0.03, 99.17)\n",
      "Epoch 18 Test set loss, accuracy (%): (0.03, 99.23)\n",
      "Epoch 19 Test set loss, accuracy (%): (0.03, 99.21)\n",
      "Epoch 20 Test set loss, accuracy (%): (0.03, 99.19)\n",
      "Run 4\n",
      "Epoch 1 Test set loss, accuracy (%): (0.07, 97.77)\n",
      "Epoch 2 Test set loss, accuracy (%): (0.04, 98.47)\n",
      "Epoch 3 Test set loss, accuracy (%): (0.04, 98.85)\n",
      "Epoch 4 Test set loss, accuracy (%): (0.03, 98.95)\n",
      "Epoch 5 Test set loss, accuracy (%): (0.03, 98.88)\n",
      "Epoch 6 Test set loss, accuracy (%): (0.03, 99.05)\n",
      "Epoch 7 Test set loss, accuracy (%): (0.03, 99.05)\n",
      "Epoch 8 Test set loss, accuracy (%): (0.03, 98.97)\n",
      "Epoch 9 Test set loss, accuracy (%): (0.03, 98.99)\n",
      "Epoch 10 Test set loss, accuracy (%): (0.03, 99.05)\n",
      "Epoch 11 Test set loss, accuracy (%): (0.04, 98.94)\n",
      "Epoch 12 Test set loss, accuracy (%): (0.04, 98.76)\n",
      "Epoch 13 Test set loss, accuracy (%): (0.07, 98.21)\n",
      "Epoch 14 Test set loss, accuracy (%): (0.03, 99.06)\n",
      "Epoch 15 Test set loss, accuracy (%): (0.03, 99.18)\n",
      "Epoch 16 Test set loss, accuracy (%): (0.04, 99.04)\n",
      "Epoch 17 Test set loss, accuracy (%): (0.03, 99.17)\n",
      "Epoch 18 Test set loss, accuracy (%): (0.03, 99.28)\n",
      "Epoch 19 Test set loss, accuracy (%): (0.03, 99.10)\n",
      "Epoch 20 Test set loss, accuracy (%): (0.03, 99.13)\n",
      "Run 5\n",
      "Epoch 1 Test set loss, accuracy (%): (0.07, 97.73)\n",
      "Epoch 2 Test set loss, accuracy (%): (0.04, 98.53)\n",
      "Epoch 3 Test set loss, accuracy (%): (0.04, 98.87)\n",
      "Epoch 4 Test set loss, accuracy (%): (0.03, 98.88)\n",
      "Epoch 5 Test set loss, accuracy (%): (0.04, 98.76)\n",
      "Epoch 6 Test set loss, accuracy (%): (0.03, 99.13)\n",
      "Epoch 7 Test set loss, accuracy (%): (0.03, 98.86)\n",
      "Epoch 8 Test set loss, accuracy (%): (0.03, 98.90)\n",
      "Epoch 9 Test set loss, accuracy (%): (0.03, 98.97)\n",
      "Epoch 10 Test set loss, accuracy (%): (0.03, 99.08)\n",
      "Epoch 11 Test set loss, accuracy (%): (0.03, 99.01)\n",
      "Epoch 12 Test set loss, accuracy (%): (0.04, 98.84)\n",
      "Epoch 13 Test set loss, accuracy (%): (0.05, 98.47)\n",
      "Epoch 14 Test set loss, accuracy (%): (0.03, 98.97)\n",
      "Epoch 15 Test set loss, accuracy (%): (0.03, 99.07)\n",
      "Epoch 16 Test set loss, accuracy (%): (0.03, 99.01)\n",
      "Epoch 17 Test set loss, accuracy (%): (0.03, 99.12)\n",
      "Epoch 18 Test set loss, accuracy (%): (0.03, 99.17)\n",
      "Epoch 19 Test set loss, accuracy (%): (0.03, 99.04)\n",
      "Epoch 20 Test set loss, accuracy (%): (0.03, 99.15)\n",
      "Test Set Mean Accuracy over 5 runs: 0.9915000796318054, Standard Deviation: 0.0005477264057844877\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "test_acc_list = []\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train)\n",
    "    for i in range(num_batches):\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "\n",
    "for runs in range(1,6):\n",
    "  print('Run {}'.format(runs))\n",
    "  batches = data_stream()\n",
    "\n",
    "  key = random.PRNGKey(123)\n",
    "  _, init_params = init_random_params(key, (-1, 28, 28, 1))\n",
    "  opt_state = opt_init(init_params)\n",
    "  itercount = itertools.count()\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    for _ in range(num_batches):\n",
    "      opt_state = update(key, next(itercount), opt_state, shape_as_image(*next(batches)))\n",
    "\n",
    "    params = get_params(opt_state)\n",
    "    test_acc = accuracy(params, shape_as_image(test_images, test_labels))\n",
    "    test_loss = loss(params, shape_as_image(test_images, test_labels))\n",
    "    print('Epoch {} Test set loss, accuracy (%): ({:.2f}, {:.2f})'.format(epoch, test_loss, 100 * test_acc))\n",
    "  test_acc_list.append(test_acc)\n",
    "mean_accuracy, std_accuracy = np.mean(np.array(test_acc_list)), np.std(np.array(test_acc_list))\n",
    "print('Test Set Mean Accuracy over 5 runs: {}, Standard Deviation: {}'.format(mean_accuracy, std_accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ECE1513H_Assignment_4_Junxi Xu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
